{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from keras.models     import Sequential\n",
    "from keras.layers     import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.reset()\n",
    "goal_steps = 500\n",
    "score_requirement = 60\n",
    "intial_games = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_a_random_game_first():\n",
    "    for step_index in range(goal_steps):\n",
    "#         env.render()\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        print(\"Step {}:\".format(step_index))\n",
    "        print(\"action: {}\".format(action))\n",
    "        print(\"observation: {}\".format(observation))\n",
    "        print(\"reward: {}\".format(reward))\n",
    "        print(\"done: {}\".format(done))\n",
    "        print(\"info: {}\".format(info))\n",
    "        if done:\n",
    "            break\n",
    "    env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0:\n",
      "action: 1\n",
      "observation: [-0.02540119  0.2303322  -0.04179892 -0.29672463]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 1:\n",
      "action: 1\n",
      "observation: [-0.02079454  0.42602432 -0.04773341 -0.60229172]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 2:\n",
      "action: 1\n",
      "observation: [-0.01227406  0.6217803  -0.05977925 -0.90961953]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 3:\n",
      "action: 0\n",
      "observation: [ 1.61548575e-04  4.27516139e-01 -7.79716371e-02 -6.36308427e-01]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 4:\n",
      "action: 0\n",
      "observation: [ 0.00871187  0.23356322 -0.09069781 -0.36916341]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 5:\n",
      "action: 0\n",
      "observation: [ 0.01338314  0.03983913 -0.09808107 -0.10640026]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 6:\n",
      "action: 0\n",
      "observation: [ 0.01417992 -0.15375045 -0.10020908  0.15379887]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 7:\n",
      "action: 1\n",
      "observation: [ 0.01110491  0.04265291 -0.0971331  -0.1687404 ]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 8:\n",
      "action: 0\n",
      "observation: [ 0.01195797 -0.15095421 -0.10050791  0.09178912]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 9:\n",
      "action: 0\n",
      "observation: [ 0.00893888 -0.34450266 -0.09867213  0.35114704]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 10:\n",
      "action: 0\n",
      "observation: [ 0.00204883 -0.53809306 -0.09164919  0.61115761]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 11:\n",
      "action: 0\n",
      "observation: [-0.00871303 -0.73182259 -0.07942603  0.87362517]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 12:\n",
      "action: 1\n",
      "observation: [-0.02334948 -0.53571575 -0.06195353  0.55706556]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 13:\n",
      "action: 1\n",
      "observation: [-0.0340638  -0.33978127 -0.05081222  0.245525  ]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 14:\n",
      "action: 0\n",
      "observation: [-0.04085942 -0.53414206 -0.04590172  0.52175753]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 15:\n",
      "action: 1\n",
      "observation: [-0.05154226 -0.33840507 -0.03546657  0.21497095]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 16:\n",
      "action: 1\n",
      "observation: [-0.05831037 -0.1427945  -0.03116715 -0.08868538]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 17:\n",
      "action: 0\n",
      "observation: [-0.06116626 -0.33745617 -0.03294086  0.19400366]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 18:\n",
      "action: 0\n",
      "observation: [-0.06791538 -0.5320918  -0.02906078  0.47611586]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 19:\n",
      "action: 1\n",
      "observation: [-0.07855722 -0.33657181 -0.01953847  0.17441705]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 20:\n",
      "action: 0\n",
      "observation: [-0.08528865 -0.53140876 -0.01605013  0.46087275]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 21:\n",
      "action: 0\n",
      "observation: [-0.09591683 -0.72630023 -0.00683267  0.74845375]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 22:\n",
      "action: 0\n",
      "observation: [-0.11044283 -0.92132726  0.0081364   1.03897868]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 23:\n",
      "action: 1\n",
      "observation: [-0.12886938 -0.72631436  0.02891598  0.7488611 ]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 24:\n",
      "action: 1\n",
      "observation: [-0.14339566 -0.53160297  0.0438932   0.46541622]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 25:\n",
      "action: 1\n",
      "observation: [-0.15402772 -0.33712783  0.05320152  0.18688492]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 26:\n",
      "action: 0\n",
      "observation: [-0.16077028 -0.532969    0.05693922  0.49586491]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 27:\n",
      "action: 1\n",
      "observation: [-0.17142966 -0.33869426  0.06685652  0.22165615]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 28:\n",
      "action: 1\n",
      "observation: [-0.17820354 -0.14458846  0.07128964 -0.0492109 ]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 29:\n",
      "action: 1\n",
      "observation: [-0.18109531  0.04944269  0.07030543 -0.31857695]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 30:\n",
      "action: 1\n",
      "observation: [-0.18010646  0.24349656  0.06393389 -0.5882859 ]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 31:\n",
      "action: 0\n",
      "observation: [-0.17523653  0.04754031  0.05216817 -0.27616828]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 32:\n",
      "action: 1\n",
      "observation: [-0.17428572  0.24188066  0.0466448  -0.55195174]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 33:\n",
      "action: 1\n",
      "observation: [-0.16944811  0.43631755  0.03560577 -0.82958126]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 34:\n",
      "action: 1\n",
      "observation: [-0.16072176  0.63093515  0.01901414 -1.11085675]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 35:\n",
      "action: 1\n",
      "observation: [-0.14810306  0.82580221 -0.00320299 -1.39751468]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 36:\n",
      "action: 0\n",
      "observation: [-0.13158701  0.63072024 -0.03115329 -1.1058349 ]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 37:\n",
      "action: 1\n",
      "observation: [-0.11897261  0.82623765 -0.05326998 -1.40812629]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 38:\n",
      "action: 0\n",
      "observation: [-0.10244785  0.63181555 -0.08143251 -1.1325608 ]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 39:\n",
      "action: 1\n",
      "observation: [-0.08981154  0.82790348 -0.10408373 -1.44963271]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 40:\n",
      "action: 1\n",
      "observation: [-0.07325347  1.02413939 -0.13307638 -1.77293897]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 41:\n",
      "action: 1\n",
      "observation: [-0.05277069  1.22048757 -0.16853516 -2.10386628]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 42:\n",
      "action: 1\n",
      "observation: [-0.02836093  1.41685245 -0.21061248 -2.44355541]\n",
      "reward: 1.0\n",
      "done: True\n",
      "info: {}\n"
     ]
    }
   ],
   "source": [
    "play_a_random_game_first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_data_preparation():\n",
    "    training_data = []\n",
    "    accepted_scores = []\n",
    "    for game_index in range(intial_games):\n",
    "        score = 0\n",
    "        game_memory = []\n",
    "        previous_observation = []\n",
    "        for step_index in range(goal_steps):\n",
    "            action = random.randrange(0, 2)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            \n",
    "            if len(previous_observation) > 0:\n",
    "                game_memory.append([previous_observation, action])\n",
    "                \n",
    "            previous_observation = observation\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "        if score >= score_requirement:\n",
    "            accepted_scores.append(score)\n",
    "            for data in game_memory:\n",
    "                if data[1] == 1:\n",
    "                    output = [0, 1]\n",
    "                elif data[1] == 0:\n",
    "                    output = [1, 0]\n",
    "                training_data.append([data[0], output])\n",
    "        \n",
    "        env.reset()\n",
    "\n",
    "    print(accepted_scores)\n",
    "    \n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[63.0, 68.0, 88.0, 63.0, 60.0, 65.0, 72.0, 62.0, 60.0, 60.0, 70.0, 60.0, 90.0, 63.0, 89.0, 83.0, 67.0, 62.0, 67.0, 68.0, 63.0, 98.0, 67.0, 113.0, 75.0, 61.0, 69.0, 85.0, 89.0, 70.0, 78.0, 60.0, 72.0, 67.0, 82.0, 81.0, 79.0, 67.0, 68.0, 66.0, 60.0, 61.0, 73.0, 70.0, 62.0, 61.0, 88.0, 89.0, 85.0, 92.0, 63.0, 60.0, 61.0, 65.0, 67.0, 69.0, 69.0, 68.0, 61.0, 67.0, 84.0, 60.0, 62.0, 69.0, 70.0, 63.0, 81.0, 68.0, 63.0, 63.0, 101.0, 61.0, 69.0, 68.0, 77.0, 63.0, 74.0, 63.0, 63.0, 67.0, 67.0, 84.0, 60.0, 93.0, 91.0, 81.0, 62.0, 65.0, 71.0, 85.0, 62.0, 63.0, 66.0, 65.0, 86.0, 61.0, 65.0, 83.0, 67.0, 73.0, 62.0, 64.0, 80.0, 61.0, 60.0, 78.0, 63.0, 64.0, 65.0, 68.0, 88.0, 74.0, 75.0, 63.0, 68.0, 68.0, 74.0, 69.0, 64.0, 71.0, 134.0, 61.0, 93.0, 72.0, 62.0, 61.0, 68.0, 68.0, 63.0, 66.0, 70.0, 82.0, 65.0, 90.0, 90.0, 80.0, 61.0, 62.0, 70.0, 64.0, 79.0, 91.0, 76.0, 69.0, 61.0, 76.0, 70.0, 61.0, 71.0, 67.0, 76.0, 93.0, 81.0, 60.0, 73.0, 61.0, 70.0, 80.0, 67.0, 92.0, 76.0, 75.0, 90.0, 65.0, 67.0, 79.0, 65.0, 85.0, 68.0]\n"
     ]
    }
   ],
   "source": [
    "training_data = model_data_preparation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_size, output_size):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=input_size, activation='relu'))\n",
    "    model.add(Dense(52, activation='relu'))\n",
    "    model.add(Dense(output_size, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer=Adam())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(training_data):\n",
    "    X = np.array([i[0] for i in training_data]).reshape(-1, len(training_data[0][0]))\n",
    "    y = np.array([i[1] for i in training_data]).reshape(-1, len(training_data[0][1]))\n",
    "    model = build_model(input_size=len(X[0]), output_size=len(y[0]))\n",
    "    \n",
    "    model.fit(X, y, epochs=10)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "11956/11956 [==============================] - 2s 193us/step - loss: 0.2442\n",
      "Epoch 2/10\n",
      "11956/11956 [==============================] - 2s 179us/step - loss: 0.2348\n",
      "Epoch 3/10\n",
      "11956/11956 [==============================] - 3s 214us/step - loss: 0.2339\n",
      "Epoch 4/10\n",
      "11956/11956 [==============================] - 3s 213us/step - loss: 0.2331\n",
      "Epoch 5/10\n",
      "11956/11956 [==============================] - 2s 174us/step - loss: 0.2327\n",
      "Epoch 6/10\n",
      "11956/11956 [==============================] - 2s 206us/step - loss: 0.2324\n",
      "Epoch 7/10\n",
      "11956/11956 [==============================] - 2s 163us/step - loss: 0.2323\n",
      "Epoch 8/10\n",
      "11956/11956 [==============================] - 2s 204us/step - loss: 0.2317\n",
      "Epoch 9/10\n",
      "11956/11956 [==============================] - 2s 206us/step - loss: 0.2316\n",
      "Epoch 10/10\n",
      "11956/11956 [==============================] - 3s 228us/step - loss: 0.2316\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_model(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500.0, 252.0, 211.0, 220.0, 261.0, 185.0, 158.0, 323.0, 197.0, 500.0, 329.0, 500.0, 213.0, 500.0, 312.0, 243.0, 157.0, 368.0, 279.0, 85.0, 148.0, 137.0, 321.0, 91.0, 263.0, 159.0, 127.0, 200.0, 500.0, 147.0, 324.0, 193.0, 263.0, 390.0, 298.0, 177.0, 500.0, 183.0, 183.0, 107.0, 113.0, 309.0, 500.0, 500.0, 164.0, 301.0, 158.0, 97.0, 500.0, 500.0, 326.0, 83.0, 205.0, 500.0, 95.0, 257.0, 500.0, 83.0, 142.0, 219.0, 125.0, 148.0, 115.0, 500.0, 500.0, 79.0, 136.0, 249.0, 95.0, 217.0, 262.0, 74.0, 74.0, 133.0, 154.0, 248.0, 500.0, 500.0, 209.0, 96.0, 500.0, 316.0, 75.0, 107.0, 229.0, 194.0, 103.0, 219.0, 83.0, 500.0, 137.0, 282.0, 252.0, 500.0, 500.0, 143.0, 170.0, 182.0, 500.0, 206.0]\n",
      "Average Score: 256.68\n",
      "choice 1:0.49594826242792583  choice 0:0.5040517375720742\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "choices = []\n",
    "for each_game in range(500):\n",
    "    score = 0\n",
    "    prev_obs = []\n",
    "    for step_index in range(goal_steps):\n",
    "        # Uncomment below line if you want to see how our bot is playing the game.\n",
    "        # env.render()\n",
    "        if len(prev_obs)==0:\n",
    "            action = random.randrange(0,2)\n",
    "        else:\n",
    "            action = np.argmax(trained_model.predict(prev_obs.reshape(-1, len(prev_obs)))[0])\n",
    "        \n",
    "        choices.append(action)\n",
    "        new_observation, reward, done, info = env.step(action)\n",
    "        prev_obs = new_observation\n",
    "        score+=reward\n",
    "        if done:\n",
    "            break\n",
    "    env.render()\n",
    "    env.reset()\n",
    "    scores.append(score)\n",
    "\n",
    "print(scores)\n",
    "print('Average Score:',sum(scores)/len(scores))\n",
    "print('choice 1:{}  choice 0:{}'.format(choices.count(1)/len(choices),choices.count(0)/len(choices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
